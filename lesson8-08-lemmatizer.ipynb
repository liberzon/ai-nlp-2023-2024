{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 22.11.1\r\n",
      "  latest version: 24.1.0\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c defaults conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.0\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -y spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:43.977992Z",
     "start_time": "2024-02-12T21:40:36.747951Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\r\n",
      "Solving environment: done\r\n",
      "\r\n",
      "\r\n",
      "==> WARNING: A newer version of conda exists. <==\r\n",
      "  current version: 22.11.1\r\n",
      "  latest version: 24.1.0\r\n",
      "\r\n",
      "Please update conda by running\r\n",
      "\r\n",
      "    $ conda update -n base -c defaults conda\r\n",
      "\r\n",
      "Or to minimize the number of packages updated during conda update use\r\n",
      "\r\n",
      "     conda install conda=24.1.0\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "# All requested packages already installed.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda install -y spacy-lookups-data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:49.780251Z",
     "start_time": "2024-02-12T21:40:43.979699Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:50.239832Z",
     "start_time": "2024-02-12T21:40:49.781448Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:50.244217Z",
     "start_time": "2024-02-12T21:40:50.239945Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemmatizer          \n",
      "friend              friend              \n",
      "friendship          friendship          \n",
      "friends             friend              \n",
      "friendships         friendship          \n",
      "stabil              stabil              \n",
      "destabilize         destabilize         \n",
      "misunderstanding    misunderstanding    \n",
      "railroad            railroad            \n",
      "moonlight           moonlight           \n",
      "football            football            \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemmatizer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}\".format(word, lemmatizer.lemmatize(word)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:50.970088Z",
     "start_time": "2024-02-12T21:40:50.244088Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks: rock\n",
      "corpora: corpus\n",
      "indices: index\n",
      "better: good\n",
      "better: better\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks:\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora:\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"indices:\", lemmatizer.lemmatize(\"indices\"))\n",
    "print(\"better:\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"better:\", lemmatizer.lemmatize(\"better\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:50.970653Z",
     "start_time": "2024-02-12T21:40:50.968638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scikit-Learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "bow_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:50.974427Z",
     "start_time": "2024-02-12T21:40:50.972460Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "sample_text = \"Apple is looking at buying U.K. startup for $1 billion\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:50.978345Z",
     "start_time": "2024-02-12T21:40:50.975267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokens = nlp(sample_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:52.524748Z",
     "start_time": "2024-02-12T21:40:50.978185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1798e2a60>),\n ('tagger', <spacy.pipeline.tagger.Tagger at 0x179ab8040>),\n ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x179ab9120>),\n ('attribute_ruler',\n  <spacy.pipeline.attributeruler.AttributeRuler at 0x179b09d00>),\n ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x179b176c0>),\n ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1796d1f20>)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:52.534149Z",
     "start_time": "2024-02-12T21:40:52.529897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['Apple',\n 'is',\n 'looking',\n 'at',\n 'buying',\n 'U.K.',\n 'startup',\n 'for',\n '$',\n '1',\n 'billion']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = []\n",
    "for token in tokens:\n",
    "    token_list.append(token.text)\n",
    "token_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:52.538063Z",
     "start_time": "2024-02-12T21:40:52.533514Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['Apple',\n 'be',\n 'look',\n 'at',\n 'buy',\n 'U.K.',\n 'startup',\n 'for',\n '$',\n '1',\n 'billion']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_list = []\n",
    "for token in tokens:\n",
    "    lemma_list.append(token.lemma_)\n",
    "lemma_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:52.542184Z",
     "start_time": "2024-02-12T21:40:52.538843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T21:40:52.542586Z",
     "start_time": "2024-02-12T21:40:52.540935Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
